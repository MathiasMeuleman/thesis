@article{Byrd2015,
   abstract = {We posit that progress in Optical Music Recognition (OMR) has been held up for years by the absence of anything resembling the standard testbeds in use in other fields that face difficult evaluation problems. One example of such a field is text information retrieval (IR), where the Text Retrieval Conference (TREC) has annually-renewed IR tasks with accompanying data sets. In music informatics, the Music Information Retrieval Exchange (MIREX), with its annual tests and meetings held during the ISMIR conference, is a close analog to TREC; but MIREX has never had an OMR track or a collection of music such a track could employ. We describe why the absence of an OMR testbed is a problem and how this problem may be mitigated. To aid in the establishment of a standard testbed, we provide (1) a set of definitions for the complexity of music notation; (2) a set of performance metrics for OMR tools that gauge score complexity and graphical quality; and (3) a small corpus of music for use as a baseline for a proper OMR testbed.},
   author = {Donald Byrd and Jakob Grue Simonsen},
   doi = {10.1080/09298215.2015.1045424},
   issn = {1744-5027},
   issue = {3},
   journal = {Journal of New Music Research},
   keywords = {empirical evaluation,notation,notation complexity,optical music recognition},
   pages = {169-195},
   title = {Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images},
   volume = {44},
   url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
   year = {2015},
}
@article{Fornes2012,
   abstract = {The analysis of music scores has been an active research field in the last decades. However, there are no publicly available databases of handwritten music scores for the research community. In this paper, we present the CVC-MUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets written by 50 different musicians. It has been especially designed for writer identification and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches.},
   author = {Alicia Fornés and Anjan Dutta and Albert Gordo and Josep Lladós},
   doi = {10.1007/s10032-011-0168-2},
   keywords = {Graphics recognition ·,Ground truths,Handwritten documents ·,Music scores ·,Performance evaluation ·,Staff removal ·,Writer identification ·},
   pages = {243-251},
   title = {CVC-MUSCIMA: a ground truth of handwritten music score images for writer identification and staff removal},
   volume = {15},
   url = {http://www.cvc.uab.es/cvcmuscima.},
   year = {2012},
}
@article{Hajic2017,
   author = {Jan Hajič and Pavel Pecina},
   journal = {14th IAPR International Conference on Document Analysis and Recognition},
   title = {The MUSCIMA++ Dataset forHandwritten Optical Music Recognition},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8269947},
   year = {2017},
}
@report{Chen2016,
   abstract = {We propose a human-in-the-loop scheme for optical music recognition. Starting from the results of our recognition engine, we pose the problem as one of constrained optimization, in which the human can specify various pixel labels, while our recognition engine seeks an optimal explanation subject to the human-supplied constraints. In this way we enable an interactive approach with a uniform communication channel from human to machine where both iterate their roles until the desired end is achieved. Pixel constraints may be added to various stages, including staff finding, system identification, and measure recognition. Results on a test show significant speed up when compared to purely human-driven correction.},
   author = {Liang Chen and Christopher Raphael},
   title = {Human-Directed Optical Music Recognition},
   year = {2016},
}
@report{CalvoZaragoza2017,
   abstract = {This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural networks. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful features of the input image, and then a recurrent block models the sequential nature of music. The system is trained using a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment between the image and the ground-truth music symbols. Experimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different possible labels. Results obtained depict classification error rates around 2 % at symbol level, thus proving the potential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes.},
   author = {Jorge Calvo-Zaragoza and Jose J Valero-Mas and Antonio Pertusa},
   title = {End-To-End Optical Music Recognition Using Neural Networks},
   url = {http://lilypond.org/},
   year = {2017},
}
@article{Gallego2017,
   abstract = {Staff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images.},
   author = {Antonio-Javier Gallego and Jorge Calvo-Zaragoza},
   doi = {10.1016/j.eswa.2017.07.002},
   journal = {Expert Systems With Applications},
   keywords = {Auto-encoders,Convolutional networks,Optical music recognition,Staff-line removal},
   pages = {138-148},
   title = {Staff-line removal with selectional auto-encoders},
   volume = {89},
   url = {http://dx.doi.org/10.1016/j.eswa.2017.07.002},
   year = {2017},
}
@report{Bellini2007,
   author = {Pierfrancesco Bellini and Ivan Bruno and Paolo Nesi},
   issue = {1},
   journal = {Source: Computer Music Journal},
   pages = {68-93},
   title = {Assessing Optical Music Recognition Tools},
   volume = {31},
   url = {https://www.jstor.org/stable/4618021?seq=1&cid=pdf-},
   year = {2007},
}
@article{Islam2017,
   abstract = {Optical Character Recognition (OCR) has been a topic of interest for many years. It is defined as the process of digitizing a document image into its constituent characters. Despite decades of intense research, developing OCR with capabilities comparable to that of human still remains an open challenge. Due to this challenging nature, researchers from industry and academic circles have directed their attentions towards Optical Character Recognition. Over the last few years, the number of academic laboratories and companies involved in research on Character Recognition has increased dramatically. This research aims at summarizing the research so far done in the field of OCR. It provides an overview of different aspects of OCR and discusses corresponding proposals aimed at resolving issues of OCR.},
   author = {Noman Islam and Zeeshan Islam and Nazia Noor},
   issue = {2},
   journal = {arXiv},
   keywords = {Character recognition,Classification,Document image analysis,OCR,OCR survey},
   month = {10},
   publisher = {arXiv},
   title = {A Survey on Optical Character Recognition System},
   volume = {10},
   url = {http://arxiv.org/abs/1710.05703},
   year = {2017},
}
@report{Zalkow2019,
   abstract = {EXTENDED ABSTRACT In this contribution, we introduce various tools that are useful in the context of score following applications, where measures are highlighted synchronously to audio playback [1,3]. Such applications require alignments between sheet music and audio representations [2]. Often, such alignments can be computed automatically in the case that the sheet music representations are given in some symbolically encoded music format. However, sheet music is often available only in the form of digitized scans. Recently, the potential of deep learning techniques has been explored to directly link pixel positions in scans of sheet music to time positions in audio recordings [1]. While being a promising research direction, such approaches still lack robustness and also require large amounts of annotated training data.},
   author = {Frank Zalkow and Angel Villar Corrales and T J Tsai and Vlora Arifi-Müller and Meinard Müller},
   title = {Tools For Semi-Automatic Bounding Box Annotation Of Musical Measures In Sheet Music},
   url = {https://www.audiolabs-erlangen.de/resources/MIR/2019-ISMIR-LBD-Measures},
   year = {2019},
}
@article{Burghardt2017,
   abstract = {In this paper, we describe the challenge of transcribing a large corpus of handwritten music scores. We conducted an evaluation study of three existing optical music recognition (OMR) tools. The evaluation results indicate that OMR approaches do not work well for our corpus of highly heterogeneous, handwritten music scores. For this reason, we designed Allegro, a web-based crowdsourcing tool that can be used to transcribe scores. We relied on a user-centered design process throughout the development cycle of the application, to ensure a high level of usability. The interface was designed in a way it can be used intuitively, even by novices of musical notation. We describe the core features of Allegro and the basic transcription workflow. A first public beta test with 18 users shows that a crowdsourced transcription approach via the Allegro tool is a viable option for the encoding of our corpus of folk songs.},
   author = {Manuel Burghardt and Sebastian Spanner},
   doi = {10.1145/3078081.3078101},
   isbn = {9781450352659},
   keywords = {H35 [Online Information Services]: Web-based services,H52 [User Interfaces]: Ergonomics, Graphical user interfaces (GUI) General Terms Performance, Design, Human Factors Keywords Optical Music Recognition, Transcription, Crowdsourcing,I75 [Document Capture]: Document analysis},
   title = {Allegro: User-centered Design of a Tool for the Crowdsourced Transcription of Handwritten Music Scores},
   url = {http://dx.doi.org/10.1145/3078081.3078101},
   year = {2017},
}
@report{Petros2020,
   abstract = {Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annota-tor transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging , and pave the way for scalable and efficient crowd-assisted music transcription systems.},
   author = {Ioannis Petros and Samiotis Sihang and Qiu Andrea Mauri and Cynthia C S Liem and Christoph Lofi and Alessandro Bozzon},
   title = {Microtask Crowdsourcing for Music Score Transcriptions: An Experiment with Error Detection},
   year = {2020},
}
@report{Waloschek2019,
   author = {Simon Waloschek and Aristotelis Hadjakos and Alexander Pacha},
   title = {Identification and Cross-Document Alignment of Measures on Music Score Images},
   url = {https://apacha.github.io/OMR-Datasets/#muscima},
   year = {2019},
}
@article{,
   abstract = {For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: few introductory materials are available, and furthermore the field has struggled with defining itself and building a shared terminology. In this tutorial, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.},
   author = {Jorge Calvo-Zaragoza and Jan Hajič and Alexander Pacha},
   doi = {10.1145/3397499},
   issue = {4},
   journal = {ACM Computing Surveys},
   keywords = {Index Terms Optical Music Recognition,Music Notation,Music Scores},
   month = {8},
   pages = {1-35},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Understanding Optical Music Recognition},
   volume = {53},
   url = {http://arxiv.org/abs/1908.03608 http://dx.doi.org/10.1145/3397499},
   year = {2019},
}
@article{Rebelo2012,
   abstract = {For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones.},
   author = {Ana Rebelo and Ichiro Fujinaga and Filipe Paszkiewicz and Andre R.S. Marcal and Carlos Guedes and Jaime S. Cardoso},
   doi = {10.1007/s13735-012-0004-6},
   issn = {2192662X},
   issue = {3},
   journal = {International Journal of Multimedia Information Retrieval},
   keywords = {Computer music,Image processing,Machine learning,Music performance},
   month = {10},
   pages = {173-190},
   publisher = {Springer London},
   title = {Optical music recognition: state-of-the-art and open issues},
   volume = {1},
   url = {http://www.finalemusic.com/.},
   year = {2012},
}
@book_section{Blostein1992,
   abstract = {The research literature concerning the automatic analysis of images of printed and handwritten music notation, for the period 1966 through 1990, is surveyed and critically examined.},
   author = {Dorothea Blostein and Henry S. Baird},
   doi = {10.1007/978-3-642-77281-8_19},
   journal = {Structured Document Image Analysis},
   note = {Horizontal barline detection should be described in here. Trouble finding a free version.<br>},
   pages = {405-434},
   publisher = {Springer Berlin Heidelberg},
   title = {A Critical Survey of Music Image Analysis},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-77281-8_19},
   year = {1992},
}
@article{Bainbridge2001,
   abstract = {This article describes the challenges posed by optical music recognition - a topic in computer science that aims to convert scanned pages of music into an on-line format. First, the problem is described; then a generalised framework for software is presented that emphasises key stages that must be solved: staff line identification, musical object location, musical feature classification, and musical semantics. Next, significant research projects in the area are reviewed, showing how each fits the generalised framework. The article concludes by discussing perhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps that help ease the task. © 2001 Kluwer Academic Publishers.},
   author = {David Bainbridge and Tim Bell},
   doi = {10.1023/A:1002485918032},
   issn = {00104817},
   issue = {2},
   journal = {Computers and the Humanities},
   keywords = {Document image analysis,Musical data acquisition,Optical music recognition,Pattern recognition},
   note = {Outlines the history of OMR research from 1960 - 2000, contains useful examples of why OMR is much more difficult than OCR.<br>},
   pages = {95-121},
   publisher = {Editura Stiintifica F. M. R.},
   title = {The challenge of optical music recognition},
   volume = {35},
   url = {https://link.springer.com/article/10.1023/A:1002485918032},
   year = {2001},
}
@article{Raphael2011,
   author = {Christopher Raphael and Jingya Wang},
   doi = {10.5281/ZENODO.1414856},
   month = {10},
   note = {Work in progress research on detecting and classifying notes and chords in IMSLP data, with an example of Beethoven sonata for violin and orchesrr},
   title = {New Approaches to Optical Music Recognition.},
   url = {https://zenodo.org/record/1414856},
   year = {2011},
}
@article{Pugin2008,
   author = {Laurent Pugin and Jason Hockman and John Ashley Burgoyne and Ichiro Fujinaga},
   doi = {10.5281/ZENODO.1417683},
   month = {9},
   title = {Gamera Versus Aruspix: Two Optical Music Recognition Approaches.},
   url = {https://zenodo.org/record/1417683},
   year = {2008},
}
@article{Wel2017,
   author = {Eelco van der Wel and Karen Ullrich},
   doi = {10.5281/ZENODO.1415664},
   month = {10},
   title = {Optical Music Recognition with Convolutional Sequence-to-Sequence Models.},
   url = {https://zenodo.org/record/1415664},
   year = {2017},
}
@article{Hajic2018,
   author = {Jan Hajič and Matthias Dorfer and Gerhard Widmer and Pavel Pecina},
   doi = {10.5281/ZENODO.1492389},
   month = {9},
   title = {Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets},
   url = {https://zenodo.org/record/1492389},
   year = {2018},
}
@article{Chen2016,
   author = {Liang Chen and Erik Stolterman and Christopher Raphael},
   doi = {10.5281/ZENODO.1416184},
   month = {8},
   title = {Human-Interactive Optical Music Recognition.},
   url = {https://zenodo.org/record/1416184},
   year = {2016},
}
@article{Parada2019,
   author = {Emilia Parada-Cabaleiro and Anton Batliner and Björn Schuller},
   doi = {10.5281/ZENODO.3527868},
   month = {11},
   title = {A Diplomatic Edition of Il Lauro Secco: Ground Truth for OMR of White Mensural Notation},
   url = {https://zenodo.org/record/3527868},
   year = {2019},
}
@article{Karsdorp2019,
   author = {Folgert Karsdorp and Peter Kranenburg and Enrique Manjavacas},
   doi = {10.5281/ZENODO.3527848},
   month = {11},
   title = {Learning Similarity Metrics for Melody Retrieval},
   url = {https://zenodo.org/record/3527848},
   year = {2019},
}
@article{Bretan2019,
   author = {Mason Bretan and Larry Heck},
   doi = {10.5281/ZENODO.3527840},
   month = {11},
   title = {Self-Supervised Methods for Learning Semantic Similarity in Music},
   url = {https://zenodo.org/record/3527840},
   year = {2019},
}
@article{Park2019,
   author = {Saebyul Park and Taegyun Kwon and Jongpil Lee and Jeounghoon Kim and Juhan Nam},
   doi = {10.5281/ZENODO.3527834},
   month = {11},
   title = {A Cross-Scape Plot Representation for Visualizing Symbolic Melodic Similarity},
   url = {https://zenodo.org/record/3527834},
   year = {2019},
}
@article{Pacha2019,
   author = {Alexander Pacha and Jorge Calvo-Zaragoza and jr. Jan Hajič},
   doi = {10.5281/ZENODO.3527744},
   month = {11},
   title = {Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition},
   url = {https://zenodo.org/record/3527744},
   year = {2019},
}
@article{Valk2019,
   author = {Reinier de Valk and Ryaan Ahmed and Tim Crawford},
   doi = {10.5281/ZENODO.3527836},
   month = {11},
   title = {JosquIntab: A Dataset for Content-based Computational Analysis of Music in Lute Tablature},
   url = {https://zenodo.org/record/3527836},
   year = {2019},
}
@article{Zitova2003,
   abstract = {This paper aims to present a review of recent as well as classic image registration methods. Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors. The registration geometrically align two images (the reference and sensed images). The reviewed approaches are classified according to their nature (area-based and feature-based) and according to four basic steps of image registration procedure: feature detection, feature matching, mapping function design, and image transformation and resampling. Main contributions, advantages, and drawbacks of the methods are mentioned in the paper. Problematic issues of image registration and outlook for the future research are discussed too. The major goal of the paper is to provide a comprehensive reference source for the researchers involved in image registration, regardless of particular application areas. © 2003 Elsevier B.V. All rights reserved.},
   author = {Barbara Zitová and Jan Flusser},
   doi = {10.1016/S0262-8856(03)00137-9},
   issn = {02628856},
   issue = {11},
   journal = {Image and Vision Computing},
   keywords = {Feature detection,Feature matching,Image registration,Mapping function,Resampling},
   pages = {977-1000},
   publisher = {Elsevier Ltd},
   title = {Image registration methods: A survey},
   volume = {21},
   year = {2003},
}
