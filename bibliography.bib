@article{Bainbridge2001,
   abstract = {This article describes the challenges posed by optical music recognition - a topic in computer science that aims to convert scanned pages of music into an on-line format. First, the problem is described; then a generalised framework for software is presented that emphasises key stages that must be solved: staff line identification, musical object location, musical feature classification, and musical semantics. Next, significant research projects in the area are reviewed, showing how each fits the generalised framework. The article concludes by discussing perhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps that help ease the task. © 2001 Kluwer Academic Publishers.},
   author = {David Bainbridge and Tim Bell},
   doi = {10.1023/A:1002485918032},
   issn = {00104817},
   issue = {2},
   journal = {Computers and the Humanities},
   keywords = {Document image analysis,Musical data acquisition,Optical music recognition,Pattern recognition},
   note = {Outlines the history of OMR research from 1960 - 2000, contains useful examples of why OMR is much more difficult than OCR.<br>},
   pages = {95-121},
   publisher = {Editura Stiintifica F. M. R.},
   title = {The challenge of optical music recognition},
   volume = {35},
   url = {https://link.springer.com/article/10.1023/A:1002485918032},
   year = {2001},
}
@report{Bellini2007,
   author = {Pierfrancesco Bellini and Ivan Bruno and Paolo Nesi},
   issue = {1},
   journal = {Source: Computer Music Journal},
   pages = {68-93},
   title = {Assessing Optical Music Recognition Tools},
   volume = {31},
   url = {https://www.jstor.org/stable/4618021?seq=1&cid=pdf-},
   year = {2007},
}
@book_section{Blostein1992,
   abstract = {The research literature concerning the automatic analysis of images of printed and handwritten music notation, for the period 1966 through 1990, is surveyed and critically examined.},
   author = {Dorothea Blostein and Henry S. Baird},
   doi = {10.1007/978-3-642-77281-8_19},
   journal = {Structured Document Image Analysis},
   note = {Horizontal barline detection should be described in here. Trouble finding a free version.<br>},
   pages = {405-434},
   publisher = {Springer Berlin Heidelberg},
   title = {A Critical Survey of Music Image Analysis},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-77281-8_19},
   year = {1992},
}
@inproceedings{Bretan2019,
   author = {Mason Bretan and Larry Heck},
   doi = {10.5281/ZENODO.3527840},
   journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference},
   month = {11},
   pages = {446-453},
   title = {Self-Supervised Methods for Learning Semantic Similarity in Music},
   url = {https://zenodo.org/record/3527840},
   year = {2019},
}
@inproceedings{Burghardt2017,
   author = {Manuel Burghardt and Sebastian Spanner},
   journal = {Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage},
   pages = {15-20},
   title = {Allegro: User-centered design of a tool for the crowdsourced transcription of handwritten music scores},
   year = {2017},
}
@article{Byrd2015,
   abstract = {We posit that progress in Optical Music Recognition (OMR) has been held up for years by the absence of anything resembling the standard testbeds in use in other fields that face difficult evaluation problems. One example of such a field is text information retrieval (IR), where the Text Retrieval Conference (TREC) has annually-renewed IR tasks with accompanying data sets. In music informatics, the Music Information Retrieval Exchange (MIREX), with its annual tests and meetings held during the ISMIR conference, is a close analog to TREC; but MIREX has never had an OMR track or a collection of music such a track could employ. We describe why the absence of an OMR testbed is a problem and how this problem may be mitigated. To aid in the establishment of a standard testbed, we provide (1) a set of definitions for the complexity of music notation; (2) a set of performance metrics for OMR tools that gauge score complexity and graphical quality; and (3) a small corpus of music for use as a baseline for a proper OMR testbed.},
   author = {Donald Byrd and Jakob Grue Simonsen},
   doi = {10.1080/09298215.2015.1045424},
   issn = {1744-5027},
   issue = {3},
   journal = {Journal of New Music Research},
   keywords = {empirical evaluation,notation,notation complexity,optical music recognition},
   pages = {169-195},
   title = {Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images},
   volume = {44},
   url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
   year = {2015},
}
@article{CalvoZaragoza2019,
   abstract = {For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: few introductory materials are available, and furthermore the field has struggled with defining itself and building a shared terminology. In this tutorial, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.},
   author = {Jorge Calvo-Zaragoza and Jan Hajič and Alexander Pacha},
   doi = {10.1145/3397499},
   issue = {4},
   journal = {ACM Computing Surveys},
   keywords = {Index Terms Optical Music Recognition,Music Notation,Music Scores},
   month = {8},
   pages = {1-35},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Understanding Optical Music Recognition},
   volume = {53},
   url = {http://arxiv.org/abs/1908.03608 http://dx.doi.org/10.1145/3397499},
   year = {2019},
}
@inproceedings{CalvoZaragoza2017,
   abstract = {This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural networks. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful features of the input image, and then a recurrent block models the sequential nature of music. The system is trained using a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment between the image and the ground-truth music symbols. Experimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different possible labels. Results obtained depict classification error rates around 2 % at symbol level, thus proving the potential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes.},
   author = {Jorge Calvo-Zaragoza and Jose J Valero-Mas and Antonio Pertusa},
   journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR},
   pages = {23-27},
   title = {End-To-End Optical Music Recognition Using Neural Networks},
   url = {http://lilypond.org/},
   year = {2017},
}
@article{CalvoZaragoza2018,
   abstract = {Optical Music Recognition is a field of research that investigates how to computationally decode music notation from images. Despite the efforts made so far, there are hardly any complete solutions to the problem. In this work, we study the use of neural networks that work in an end-to-end manner. This is achieved by using a neural model that combines the capabilities of convolutional neural networks, which work on the input image, and recurrent neural networks, which deal with the sequential nature of the problem. Thanks to the use of the the so-called Connectionist Temporal Classification loss function, these models can be directly trained from input images accompanied by their corresponding transcripts into music symbol sequences. We also present the Printed Images of Music Staves (PrIMuS) dataset, containing more than 80,000 monodic single-staff real scores in common western notation, that is used to train and evaluate the neural approach. In our experiments, it is demonstrated that this formulation can be carried out successfully. Additionally, we study several considerations about the codification of the output musical sequences, the convergence and scalability of the neural models, as well as the ability of this approach to locate symbols in the input score.},
   author = {Jorge Calvo-Zaragoza and David Rizo},
   doi = {10.3390/app8040606},
   issue = {4},
   journal = {Applied Sciences},
   keywords = {Deep Learning,Optical Music Recognition,end-to-end recognition,music score images},
   title = {End-to-End Neural Optical Music Recognition of Monophonic Scores},
   volume = {8},
   url = {https://musescore.org},
   year = {2018},
}
@inproceedings{Chen2016b,
   author = {Liang Chen and Erik Stolterman and Christopher Raphael},
   doi = {10.5281/ZENODO.1416184},
   journal = {ISMIR},
   month = {8},
   pages = {647-653},
   title = {Human-Interactive Optical Music Recognition.},
   url = {https://zenodo.org/record/1416184},
   year = {2016},
}
@article{Chen2016,
   abstract = {We propose a human-in-the-loop scheme for optical music recognition. Starting from the results of our recognition engine, we pose the problem as one of constrained optimization, in which the human can specify various pixel labels, while our recognition engine seeks an optimal explanation subject to the human-supplied constraints. In this way we enable an interactive approach with a uniform communication channel from human to machine where both iterate their roles until the desired end is achieved. Pixel constraints may be added to various stages, including staff finding, system identification, and measure recognition. Results on a test show significant speed up when compared to purely human-driven correction.},
   author = {Liang Chen and Christopher Raphael},
   issue = {17},
   journal = {Electronic Imaging},
   pages = {1-9},
   title = {Human-Directed Optical Music Recognition},
   volume = {2016},
   year = {2016},
}
@article{Dalitz2008,
   author = {Christoph Dalitz and Michael Droettboom and Bastian Pranzas and Ichiro Fujinaga},
   issue = {5},
   journal = {IEEE transactions on pattern analysis and machine intelligence},
   pages = {753-766},
   publisher = {IEEE},
   title = {A comparative study of staff removal algorithms},
   volume = {30},
   year = {2008},
}
@article{Fornes2012,
   abstract = {The analysis of music scores has been an active research field in the last decades. However, there are no publicly available databases of handwritten music scores for the research community. In this paper, we present the CVC-MUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets written by 50 different musicians. It has been especially designed for writer identification and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches.},
   author = {Alicia Fornés and Anjan Dutta and Albert Gordo and Josep Lladós},
   doi = {10.1007/s10032-011-0168-2},
   issue = {3},
   journal = {International Journal on Document Analysis and Recognition (IJDAR)},
   keywords = {Graphics recognition ·,Ground truths,Handwritten documents ·,Music scores ·,Performance evaluation ·,Staff removal ·,Writer identification ·},
   pages = {243-251},
   title = {CVC-MUSCIMA: a ground truth of handwritten music score images for writer identification and staff removal},
   volume = {15},
   url = {http://www.cvc.uab.es/cvcmuscima.},
   year = {2012},
}
@thesis{Fujinaga1996,
   author = {Ichiro Fujinaga},
   city = {Montreal},
   institution = {McGill University},
   month = {6},
   title = {Adaptive Optical Music Recognition},
   url = {moz-extension://b1bc1f1d-22b8-4225-8d70-0b4c95e8cc68/enhanced-reader.html?openApp&pdf=http%3A%2F%2Fwww.music.mcgill.ca%2F~ich%2Fresearch%2Fdiss%2FFujinagaDiss.pdf},
   year = {1996},
}
@article{Gallego2017,
   abstract = {Staff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images.},
   author = {Antonio-Javier Gallego and Jorge Calvo-Zaragoza},
   doi = {10.1016/j.eswa.2017.07.002},
   journal = {Expert Systems With Applications},
   keywords = {Auto-encoders,Convolutional networks,Optical music recognition,Staff-line removal},
   pages = {138-148},
   title = {Staff-line removal with selectional auto-encoders},
   volume = {89},
   url = {http://dx.doi.org/10.1016/j.eswa.2017.07.002},
   year = {2017},
}
@inproceedings{Hajic2018,
   author = {Jan Hajič and Matthias Dorfer and Gerhard Widmer and Pavel Pecina},
   doi = {10.5281/ZENODO.1492389},
   journal = {ISMIR},
   month = {9},
   pages = {225-232},
   title = {Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets},
   url = {https://zenodo.org/record/1492389},
   year = {2018},
}
@article{Hajic2017,
   author = {Jan Hajič and Pavel Pecina},
   journal = {14th IAPR International Conference on Document Analysis and Recognition},
   title = {The MUSCIMA++ Dataset forHandwritten Optical Music Recognition},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8269947},
   year = {2017},
}
@inproceedings{Homenda2006,
   abstract = {This paper presents a pattern recognition study aimed at music symbols recognition. The study is focused on classification methods of music symbols based on decision trees and clustering method     applied to classes of music symbols that face classification problems. Classification is made on the basis of extracted features. A comparison of selected classifiers was made on some classes of notation sym    bols distorted by a variety of factors as image noise, printing defects, different fonts, skew and curvature of scanning, overlapped symbols. © 2006 IEEE.},
   author = {Wladyslaw Homenda and Marcin Luckner},
   doi = {10.1109/ijcnn.2006.247339},
   isbn = {0780394909},
   issn = {10987576},
   journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
   pages = {3382-3388},
   title = {Automatic knowledge acquisition: Recognizing music notation with methods of centroids and classifications trees},
   year = {2006},
}
@inproceedings{Macmillan2002,
   abstract = {An optical music recognition system has been completely overhauled and reformatted into a new framework called Gamera. The new open-source software is not only designed to recognize various music notations, including handwritten scores, but can be used to develop systems that can recognize many other structured documents. Gamera is intended to be used by domain experts with particular knowledge of the documents to be recognized but without strong programming skills. Gamera contains image processing and recognition tools in an easy-to-use, interactive, graphical scripting environment. Additionally, the system can be extended through a C++ and Python plugins.},
   author = {Karl Macmillan and Michael Droettboom and Ichiro Fujinaga},
   journal = {ICMC},
   keywords = {optical music recognition},
   title = {Gamera: Optical music recognition in a new shell},
   url = {https://www.researchgate.net/publication/255637263},
   year = {2002},
}
@article{Otsu1979,
   author = {Nobuyuki Otsu},
   issue = {1},
   journal = {IEEE transactions on systems, man, and cybernetics},
   pages = {62-66},
   publisher = {IEEE},
   title = {A threshold selection method from gray-level histograms},
   volume = {9},
   year = {1979},
}
@inproceedings{Pacha2019,
   author = {Alexander Pacha and Jorge Calvo-Zaragoza and Jan Hajič jr.},
   doi = {10.5281/ZENODO.3527744},
   journal = {ISMIR},
   month = {11},
   pages = {75-82},
   title = {Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition},
   url = {https://zenodo.org/record/3527744},
   year = {2019},
}
@inproceedings{Pugin2008,
   author = {Laurent Pugin and Jason Hockman and John Ashley Burgoyne and Ichiro Fujinaga},
   doi = {10.5281/ZENODO.1417683},
   journal = {ISMIR 2008--Session 3C--OMR, Alignment and Annotation},
   month = {9},
   title = {Gamera Versus Aruspix: Two Optical Music Recognition Approaches.},
   url = {https://zenodo.org/record/1417683},
   year = {2008},
}
@inproceedings{Raphael2011,
   author = {Christopher Raphael and Jingya Wang},
   doi = {10.5281/ZENODO.1414856},
   journal = {ISMIR},
   month = {10},
   note = {Work in progress research on detecting and classifying notes and chords in IMSLP data, with an example of Beethoven sonata for violin and orchesrr},
   pages = {305-310},
   title = {New Approaches to Optical Music Recognition.},
   url = {https://zenodo.org/record/1414856},
   year = {2011},
}
@article{Rebelo2012,
   abstract = {For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones.},
   author = {Ana Rebelo and Ichiro Fujinaga and Filipe Paszkiewicz and Andre R.S. Marcal and Carlos Guedes and Jaime S. Cardoso},
   doi = {10.1007/s13735-012-0004-6},
   issn = {2192662X},
   issue = {3},
   journal = {International Journal of Multimedia Information Retrieval},
   keywords = {Computer music,Image processing,Machine learning,Music performance},
   month = {10},
   pages = {173-190},
   publisher = {Springer London},
   title = {Optical music recognition: state-of-the-art and open issues},
   volume = {1},
   url = {http://www.finalemusic.com/.},
   year = {2012},
}
@inproceedings{Samiotis2020,
   abstract = {Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annota-tor transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging , and pave the way for scalable and efficient crowd-assisted music transcription systems.},
   author = {Ioannis Petros Samiotis and Sihang Qiu and Andrea Mauri and Cynthia C S Liem and Christoph Lofi and Alessandro Bozzon},
   journal = {Proceedings of the 21st International Society for Music Information Retrieval Conference},
   title = {Microtask Crowdsourcing for Music Score Transcriptions: An Experiment with Error Detection},
   year = {2020},
}
@inproceedings{Vigliensoni2013,
   abstract = {This paper presents work on the automatic recognition of measures in common Western music notation scores using optical music recognition techniques. It is important to extract the bounding boxes of measures within a music score to facilitate some methods of multimodal navigation of music catalogues. We present an image processing algorithm that extracts the position of barlines on an input music score in order to deduce the number and position of measures on the page. An open-source implementation of this algorithm is made publicly available. In addition, we have created a ground-truth dataset of 100 images of music scores with manually annotated measures. We conducted several experiments using different combinations of values for two critical parameters to evaluate our measure recognition algorithm. Our algorithm obtained an f-score of 91 percent with the optimal set of parameters. Although our implementation obtained results similar to previous approaches , the scope and size of the evaluation dataset is significantly larger.},
   author = {Gabriel Vigliensoni and Gregory Burlet and Ichiro Fujinaga},
   journal = {ISMIR},
   pages = {125-130},
   title = {Optical Measure Recognition In Common Music Notation},
   url = {https://github.com/DDMAL/},
   year = {2013},
}
@inproceedings{Waloschek2019,
   author = {Simon Waloschek and Aristotelis Hadjakos and Alexander Pacha},
   journal = {ISMIR},
   pages = {137-143},
   title = {Identification and Cross-Document Alignment of Measures on Music Score Images},
   url = {https://apacha.github.io/OMR-Datasets/#muscima},
   year = {2019},
}
@inproceedings{Wel2017,
   author = {Eelco van der Wel and Karen Ullrich},
   doi = {10.5281/ZENODO.1415664},
   journal = {18th International Society for Music Information Retrieval Conference},
   month = {10},
   title = {Optical Music Recognition with Convolutional Sequence-to-Sequence Models.},
   url = {https://zenodo.org/record/1415664},
   year = {2017},
}
@inproceedings{Zalkow2019,
   abstract = {EXTENDED ABSTRACT In this contribution, we introduce various tools that are useful in the context of score following applications, where measures are highlighted synchronously to audio playback [1,3]. Such applications require alignments between sheet music and audio representations [2]. Often, such alignments can be computed automatically in the case that the sheet music representations are given in some symbolically encoded music format. However, sheet music is often available only in the form of digitized scans. Recently, the potential of deep learning techniques has been explored to directly link pixel positions in scans of sheet music to time positions in audio recordings [1]. While being a promising research direction, such approaches still lack robustness and also require large amounts of annotated training data.},
   author = {Frank Zalkow and Angel Villar Corrales and T J Tsai and Vlora Arifi-Müller and Meinard Müller},
   journal = {20th Conference of ISMIR},
   title = {Tools For Semi-Automatic Bounding Box Annotation Of Musical Measures In Sheet Music},
   url = {https://www.audiolabs-erlangen.de/resources/MIR/2019-ISMIR-LBD-Measures},
   year = {2019},
}
@article{,
   abstract = {This paper aims to present a review of recent as well as classic image registration methods. Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors. The registration geometrically align two images (the reference and sensed images). The reviewed approaches are classified according to their nature (area-based and feature-based) and according to four basic steps of image registration procedure: feature detection, feature matching, mapping function design, and image transformation and resampling. Main contributions, advantages, and drawbacks of the methods are mentioned in the paper. Problematic issues of image registration and outlook for the future research are discussed too. The major goal of the paper is to provide a comprehensive reference source for the researchers involved in image registration, regardless of particular application areas. © 2003 Elsevier B.V. All rights reserved.},
   author = {Barbara Zitová and Jan Flusser},
   doi = {10.1016/S0262-8856(03)00137-9},
   issn = {02628856},
   issue = {11},
   journal = {Image and Vision Computing},
   keywords = {Feature detection,Feature matching,Image registration,Mapping function,Resampling},
   pages = {977-1000},
   publisher = {Elsevier Ltd},
   title = {Image registration methods: A survey},
   volume = {21},
   year = {2003},
}
@article{Senin2008,
   author = {Pavel Senin},
   issue = {40},
   journal = {Information and Computer Science Department University of Hawaii at Manoa Honolulu, USA},
   pages = {1-23},
   title = {Dynamic Time Warping Algorithm Review},
   volume = {855},
   year = {2008},
}
@inproceedings{Niennattrakul2006,
   author = {Vit Niennattrakul and Chotirat Ann Ratanamahatana},
   journal = {2006 International Conference on Hybrid Information Technology},
   pages = {372-379},
   title = {Clustering Multimedia Data Using Time Series},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021117},
   year = {2006},
}
@inproceedings{Niennattrakul2007,
   author = {Vit Niennattrakul and Chotirat Ann Ratanamahatana},
   journal = {2007 International Conference on Multimedia and Ubiquitous Engineering (MUE'07)},
   pages = {733-738},
   title = {On Clustering Multimedia Time Series Data Using K-Means and  Dynamic Time Warping},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197360},
   year = {2007},
}
@article{Prerau1992,
   author = {D Prerau},
   journal = {D. Blostein and H. Baird. A Critical Survey of Music Image Analysis. In Structured Document Image Analysis},
   pages = {405-434},
   title = {Computer pattern recognition of standard engraved music notation, 1970},
   year = {1992},
}
@article{Pruslin1992,
   author = {D Pruslin},
   journal = {Structured document image analysis. Springer, Heidelberg},
   pages = {405-434},
   title = {Automatic recognition of sheet music, 1966. A critical survey of music image analysis},
   year = {1992},
}
@misc{TROMPAwebpage,
    title = {TROMPA website - About},
    howpublished = {\url{https://trompamusic.eu/about-trompa}},
    note = {Accessed: 2021-05-03},
}
@misc{IMSLP,
    title = {IMSLP website - Main page},
    howpublished = {\url{https://imslp.org/wiki/Main_Page}},
    note = {Accessed: 2021-05-03},
}
